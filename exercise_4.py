# -*- coding: utf-8 -*-
"""exercise_4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LkdzvgmvZbiiv7-jKq0oa5zVzmQrKcjh
"""

import pip
from google.colab import drive
drive.mount('/content/drive')

"""Potential important resource:
https://stellargraph.readthedocs.io/en/stable/demos/embeddings/graphsage-unsupervised-sampler-embeddings.html#Extracting-node-embeddings

## Exercise 4:
- Run graphsage again on the cora data, but this time use the embeddings as the input of some similarity function that allows you to condense the graph.
- Find communities on the cora data with label percolation and use them to condense the graph. Compare it visually with the results of the previous approach.
"""

pip install tqdm

import sys
sys.path.append('/content/drive/MyDrive/ADVANCED_NETWORK_SCIENCE/exercise_4')
from graphsage.encoders import Encoder
from graphsage.aggregators import MeanAggregator

sys.path.append('/content/drive/MyDrive/ADVANCED_NETWORK_SCIENCE/exercise_4/cora')

"""# IMPORTS"""

import torch, sys, random
import numpy as np
import pandas as pd
import torch.nn as nn
import networkx as nx
from sklearn.manifold import TSNE
from torch.autograd import Variable
from sklearn.metrics import f1_score
from graphsage.encoders import Encoder
from graphsage.aggregators import MeanAggregator
from matplotlib import pyplot as plt
from collections import Counter
from tqdm import tqdm
import pandas as pd
from sklearn.decomposition import PCA

"""# GRAPHSAGE'S NODE ENCODER"""

import torch
import torch.nn as nn
from torch.nn import init
import torch.nn.functional as F

class Encoder(nn.Module):
    """
    Encodes a node's using 'convolutional' GraphSage approach
    """
    def __init__(self, features, feature_dim,
            embed_dim, adj_lists, aggregator,
            num_sample=10,
            base_model=None, gcn=False, cuda=False,
            feature_transform=False):
        super(Encoder, self).__init__()

        self.features = features
        self.feat_dim = feature_dim
        self.adj_lists = adj_lists
        self.aggregator = aggregator
        self.num_sample = num_sample
        if base_model != None:
            self.base_model = base_model

        self.gcn = gcn
        self.embed_dim = embed_dim
        self.cuda = cuda
        self.aggregator.cuda = cuda
        self.weight = nn.Parameter(
                torch.FloatTensor(embed_dim, self.feat_dim if self.gcn else 2 * self.feat_dim))
        init.xavier_uniform_(self.weight)

    def forward(self, nodes):
        """
        Generates embeddings for a batch of nodes.

        nodes     -- list of nodes
        """
        neigh_feats = self.aggregator.forward(nodes, [self.adj_lists[int(node)] for node in nodes],
                self.num_sample)
        if not self.gcn:
            if self.cuda:
                self_feats = self.features(torch.LongTensor(nodes).cuda())
            else:
                self_feats = self.features(torch.LongTensor(nodes))
            combined = torch.cat([self_feats, neigh_feats], dim=1)
        else:
            combined = neigh_feats
        combined = F.relu(self.weight.mm(combined.t()))
        return combined

"""# DEFINING A SUPERVISED GRAPH SAGE MODEL

- Inputs = number of classes, encoder ('enc')
- Methods = forward to compute node embeddings, loss to calculate cross entropy loss
"""

class SupervisedGraphSage(nn.Module):
    def __init__(self, num_classes, enc):
        super(SupervisedGraphSage, self).__init__()
        self.enc = enc
        self.xent = nn.CrossEntropyLoss()
        self.weight = nn.Parameter(torch.FloatTensor(num_classes, enc.embed_dim))
        nn.init.xavier_uniform_(self.weight)

    def forward(self, nodes):
        embeds = self.enc(nodes)
        scores = self.weight.mm(embeds)
        return scores.t()

    def loss(self, nodes, labels):
        scores = self.forward(nodes)
        return self.xent(scores, labels.squeeze().to("cuda"))

"""# LOADING THE CORA DATASET (data, labels, graph structure)

- In addition, the function turns the df into an adjacency list of the graph
- And returns feature data (feat_data), labels (labels), adjacency lists (adj_lists) and a mapping of the nodes IDs to indices (node_map)
"""

def load_cora():
   df = pd.read_csv("/content/drive/MyDrive/ADVANCED_NETWORK_SCIENCE/exercise_4/cora/cora.content", sep = "\t", header = None)
   feat_data = df.values[:,1:-1].astype(float)
   labels = pd.factorize(df.iloc[:,-1])[0]
   node_map = df.reset_index().set_index(0)["index"].to_dict()
   G = nx.read_edgelist("/content/drive/MyDrive/ADVANCED_NETWORK_SCIENCE/exercise_4/cora/cora.cites", nodetype = int)
   adj_lists = {}
   for n in G.nodes():
      adj_lists[node_map[n]] = {node_map[n2] for n2 in G.neighbors(n)}
   return G, feat_data, labels, adj_lists, {node_map[n]: n for n in node_map}

G, feat_data, labels, adj_lists, node_map = load_cora()

"""# nn.Embedding module to create an embedding layer"""

# here I am creating graph embeddings from the feat_data (shape 2708, 1433), each one with length 1433 and 1 dimension
# feat_data = 2708 documents (nodes),
            # 1433 words (features)
features = nn.Embedding(feat_data.shape[0], feat_data.shape[1]) # the features are NOT the embeddings!!
features.weight = nn.Parameter(torch.FloatTensor(feat_data), requires_grad = False)
features.cuda()

agg1 = MeanAggregator(features, cuda = True)
enc1 = Encoder(features, feat_data.shape[1], 128, adj_lists, agg1, gcn = True, cuda = True) # this sets the size of the embedding to be 128
agg2 = MeanAggregator(lambda nodes : enc1(nodes).t(), cuda = True)
enc2 = Encoder(lambda nodes : enc1(nodes).t(), enc1.embed_dim, 128, adj_lists, agg2, base_model = enc1, gcn = True, cuda = True)
enc1.num_samples = 5
enc2.num_samples = 5

"""# Training

- Randomly select a batch of training nodes
- Compute the loss (predictions vs true labels)
- Perform backpropagation using SGD
"""

graphsage = SupervisedGraphSage(len(set(labels)), enc2)
graphsage.cuda()
rand_indices = np.random.permutation(feat_data.shape[0])
test = rand_indices[:1000]
val = rand_indices[1000:1500]
train = list(rand_indices[1500:])

optimizer = torch.optim.SGD(filter(lambda p : p.requires_grad, graphsage.parameters()), lr = 0.7)
for batch in range(100):
   batch_nodes = train[:256]
   random.shuffle(train)
   optimizer.zero_grad()
   loss = graphsage.loss(batch_nodes, Variable(torch.LongTensor(labels[np.array(batch_nodes)])))
   loss.backward()
   optimizer.step()
   sys.stderr.write(f"Batch {batch}, Loss {loss.data.item()}\n")

"""# F1 Evaluation"""

val_output = graphsage.forward(val)
sys.stderr.write(f"Validation F1: {f1_score(labels[val], val_output.data.cpu().numpy().argmax(axis = 1), average = 'micro')}\n")

test_output = graphsage.forward(test)
sys.stderr.write(f"Test F1: {f1_score(labels[test], test_output.data.cpu().numpy().argmax(axis = 1), average = 'micro')}\n")

"""# Retrieving embeddings and finding similarities based on similarity function"""

embeds = graphsage.enc(sorted(rand_indices)) # embeddings retrieved from the encoder function
preds = graphsage.forward(sorted(rand_indices)) # predicions retrieved from the forward function
tsne_data = TSNE(n_components=2,init='random').fit_transform(embeds.T.cpu().detach().numpy()) # dimensionality reduction of the embeddings (exercise 2)
pred_labels = np.argmax(preds.detach().cpu().numpy(),axis=1)

embeds.t().shape
# 2708 documents (nodes),
# 128 reduced features (words)

pred_labels

"""### This snippet is part of exercise 2 but helps understand the concepts"""

# visualizations from exercise 2
fig,ax = plt.subplots(figsize=(16,9))
pos = {ind:point for point, ind in zip(tsne_data,G.nodes)}
node_color_dict = {0:[1,0,0],1:[0,1,0],2:[0,0,1],3:[0,1,1],4:[1,0,1],5:[1,1,0],6:[0.5,0.5,1]}
node_color = [node_color_dict[pred] for pred in pred_labels]
true_node_color = [node_color_dict[pred] for pred in labels]
print(Counter(pred_labels))

nx.draw(G,pos=pos,ax=ax,node_size=50,alpha=0.5,node_color=node_color)
fig.savefig('temp.svg')
print('first')

fig2,ax2 = plt.subplots(figsize=(16,9))
nx.draw(G,ax=ax2,pos=nx.spring_layout(G),node_size=50,alpha=0.5,node_color=node_color)
fig2.savefig('temp2.svg')
print('second')

fig3,ax3 = plt.subplots(figsize=(16,9))
nx.draw(G,ax=ax3,pos=pos,node_size=50,alpha=0.5,node_color=true_node_color)
fig3.savefig('temp3.svg')
print('third')

from networkx.drawing.nx_pylab import kamada_kawai_layout
def manhattan_distance(node_embeddings, threshold_value):
    # manhattan distance
    manhattan_distances = torch.cdist(node_embeddings, node_embeddings, p=1)
    # Define a threshold to determine edge connections
    threshold = threshold_value
    # Create a new empty graph for the condensed graph
    F = nx.Graph()
    # Add nodes to the condensed graph
    num_nodes = len(node_embeddings)
    F.add_nodes_from(range(num_nodes))
    # Add edges to the condensed graph based on the threshold
    for i in tqdm(range(num_nodes)):
        for j in range(i + 1, num_nodes):
            if manhattan_distances[i, j] <= threshold:
                F.add_edge(i, j)
    # visualization
    fig,ax = plt.subplots(figsize=(10,9))
    pos = kamada_kawai_layout(F)
    node_color_dict = {0:[1,0,0],1:[0,1,0],2:[0,0,1],3:[0,1,1],4:[1,0,1],5:[1,1,0],6:[0.5,0.5,1]}
    node_color = [node_color_dict[pred] for pred in pred_labels]

    nx.draw(F,pos=pos,ax=ax,node_size=50,alpha=0.5,node_color=node_color)
    print(len(F.nodes))
  # Display the graph
    plt.show()

transposed_embeds = embeds.T.cpu().detach()
transposed_embeds.shape

# 2708 nodes
# 128 features - I am reducing these features using PCA for computational ease

pca = PCA(n_components = 10)

# Fit the PCA model to your data
pca.fit(transposed_embeds)

# Transform the data to the lower-dimensional space
pca_data = pca.transform(transposed_embeds)

principalDf = pd.DataFrame(data = pca_data
             , columns = ['principal component 1', 'principal component 2', 'principal component 3',
                          'principal component 4', 'principal component 5', 'principal component 6',
                          'principal component 7', 'principal component 8', 'principal component 9',
                          'principal component 10'])

principalDf

torch.cdist(transposed_embeds, transposed_embeds, p=1).shape

torch.cdist(transposed_embeds, transposed_embeds, p=1)

# manhattan distance
manhattan_distances = torch.cdist(torch.FloatTensor(pca_data), torch.FloatTensor(pca_data), p=1)
# Define a threshold to determine edge connections
threshold = 30
# Create a new empty graph for the condensed graph
F = nx.Graph()
# Add nodes to the condensed graph
num_nodes = len(pca_data)
F.add_nodes_from(range(num_nodes))
    # Add edges to the condensed graph based on the threshold
for i in tqdm(range(num_nodes)):
    for j in range(i + 1, num_nodes):
        if manhattan_distances[i, j] <= threshold:
            F.add_edge(i, j)

# visualization
fig,ax = plt.subplots(figsize=(10,9))
pos = {ind: (point[0], point[1]) for ind, point in enumerate(pca_data)}
node_color_dict = {0:[1,0,0],1:[0,1,0],2:[0,0,1],3:[0,1,1],4:[1,0,1],5:[1,1,0],6:[0.5,0.5,1]}
node_color = [node_color_dict[pred] for pred in pred_labels]

nx.draw(F,pos=pos,ax=ax,node_size=50,alpha=0.5,node_color=node_color)
# Display the graph
plt.show()